{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPmr6vK+QjaZeoqK2JB7QA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51937/23CSBTB27-28/blob/main/Assignment3(R).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0XcqjabBSYE",
        "outputId": "570dc8ac-600b-4a06-9ccf-bc8dca9bf793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Monte Carlo Policy Evaluation ===\n",
            "V(0) = 0.759\n",
            "V(1) = 0.900\n",
            "V(2) = 1.000\n",
            "V(3) = 0.000\n",
            "\n",
            "=== Monte Carlo Control (ε-greedy) ===\n",
            "State 0: Q = [0.71405064 0.80219979]\n",
            "State 1: Q = [0.71891085 0.9       ]\n",
            "State 2: Q = [0.80014693 1.        ]\n",
            "Learned Policy: {0: np.int64(1), 1: np.int64(1), 2: np.int64(1)}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# -------------------------------\n",
        "# Toy MDP Definition\n",
        "# -------------------------------\n",
        "states = [0, 1, 2, 3]   # terminal = 3\n",
        "actions = [0, 1]        # 0 = left, 1 = right\n",
        "gamma = 0.9\n",
        "\n",
        "# Transition model: P[s][a] -> (next_state, reward, done)\n",
        "P = {\n",
        "    0: {0: (0, 0, False), 1: (1, 0, False)},\n",
        "    1: {0: (0, 0, False), 1: (2, 0, False)},\n",
        "    2: {0: (1, 0, False), 1: (3, 1, True)},   # reward when reaching state 3\n",
        "    3: {0: (3, 0, True),  1: (3, 0, True)}    # terminal\n",
        "}\n",
        "\n",
        "# -------------------------------\n",
        "# Environment Functions\n",
        "# -------------------------------\n",
        "def step(state, action):\n",
        "    \"\"\"Return next_state, reward, done given current state and action.\"\"\"\n",
        "    return P[state][action]\n",
        "\n",
        "def generate_episode(policy):\n",
        "    \"\"\"Generate an episode following a given policy function.\"\"\"\n",
        "    episode = []\n",
        "    state = 0  # start always from state 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy(state)\n",
        "        next_state, reward, done = step(state, action)\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    return episode\n",
        "\n",
        "# -------------------------------\n",
        "# Monte Carlo Policy Evaluation\n",
        "# -------------------------------\n",
        "def mc_policy_evaluation(policy, episodes=5000, gamma=0.9):\n",
        "    V = defaultdict(float)\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        episode = generate_episode(policy)\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for state, action, reward in reversed(episode):\n",
        "            G = gamma * G + reward\n",
        "            if state not in visited:  # first-visit MC\n",
        "                returns_sum[state] += G\n",
        "                returns_count[state] += 1\n",
        "                V[state] = returns_sum[state] / returns_count[state]\n",
        "                visited.add(state)\n",
        "    return dict(V)\n",
        "\n",
        "# -------------------------------\n",
        "# Monte Carlo Control (ε-greedy)\n",
        "# -------------------------------\n",
        "def mc_control_epsilon_greedy(episodes=10000, gamma=0.9, epsilon=0.1):\n",
        "    Q = defaultdict(lambda: np.zeros(len(actions)))\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "\n",
        "    def epsilon_greedy_policy(state):\n",
        "        if random.random() < epsilon:\n",
        "            return random.choice(actions)\n",
        "        else:\n",
        "            return np.argmax(Q[state])\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        episode = []\n",
        "        state = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = epsilon_greedy_policy(state)\n",
        "            next_state, reward, done = step(state, action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "\n",
        "        # update Q-values\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for state, action, reward in reversed(episode):\n",
        "            G = gamma * G + reward\n",
        "            if (state, action) not in visited:\n",
        "                key = (state, action)\n",
        "                returns_sum[key] += G\n",
        "                returns_count[key] += 1\n",
        "                Q[state][action] = returns_sum[key] / returns_count[key]\n",
        "                visited.add((state, action))\n",
        "\n",
        "    # derive greedy policy\n",
        "    policy = {s: np.argmax(Q[s]) for s in Q.keys()}\n",
        "    return dict(Q), policy\n",
        "\n",
        "# -------------------------------\n",
        "# Example Run\n",
        "# -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Random policy for evaluation\n",
        "    def random_policy(state):\n",
        "        return random.choice(actions)\n",
        "\n",
        "    print(\"=== Monte Carlo Policy Evaluation ===\")\n",
        "    V = mc_policy_evaluation(random_policy, episodes=5000)\n",
        "    for s in states:\n",
        "        print(f\"V({s}) = {V.get(s, 0):.3f}\")\n",
        "\n",
        "    print(\"\\n=== Monte Carlo Control (ε-greedy) ===\")\n",
        "    Q, policy = mc_control_epsilon_greedy(episodes=20000, epsilon=0.1)\n",
        "    for s in sorted(Q.keys()):\n",
        "        print(f\"State {s}: Q = {Q[s]}\")\n",
        "    print(\"Learned Policy:\", policy)\n"
      ]
    }
  ]
}